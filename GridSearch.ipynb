{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def grid_search_classifiers(classifiers, param_grids, X_train, y_train):\n",
    "    best_params = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        grid_search = GridSearchCV(clf, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_params[name] = grid_search.best_params_\n",
    "    return best_params\n",
    "\n",
    "# Define the parameter grids for each classifier\n",
    "param_grids = {\n",
    "    'logistic regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto', 0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'sigmoid']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [10, 50, 100, 200],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Find the best parameters for each classifier\n",
    "best_params = grid_search_classifiers(classifiers, param_grids, X_train, y_train)\n",
    "\n",
    "# Update classifiers with the best parameters\n",
    "for name, clf in classifiers.items():\n",
    "    clf.set_params(**best_params[name])\n",
    "\n",
    "# Evaluate classifiers with the new parameters\n",
    "classifier_results = evaluate_classifiers(classifiers, X_train, X_test, y_train, y_test)\n",
    "\n",
    "for name, accuracy in classifier_results.items():\n",
    "    print(f\"{name}: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (xgboost.dll) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): ['[WinError 193] %1 is not a valid Win32 application']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-37afea842059>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m from .core import (\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mDMatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mDeviceQuantileDMatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;31m# load the XGBoost library globally\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m \u001b[0m_LIB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[1;34m()\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[0mError\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mos_error_list\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m \"\"\"\n\u001b[0m\u001b[0;32m    198\u001b[0m         )\n\u001b[0;32m    199\u001b[0m     \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: \nXGBoost Library (xgboost.dll) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): ['[WinError 193] %1 is not a valid Win32 application']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'logistic regression': LogisticRegression(random_state=42),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'LightGBM': LGBMClassifier(random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(random_state=42, verbose=0)\n",
    "}\n",
    "\n",
    "# Define parameter grids for grid search\n",
    "param_grids = {\n",
    "    'logistic regression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]  # Inverse of regularization strength\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],  # Regularization parameter\n",
    "        'gamma': ['scale', 'auto', 0.1, 1, 10, 100],  # Kernel coefficient\n",
    "        'kernel': ['linear', 'rbf', 'sigmoid']  # Kernel type\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [10, 50, 100, 200],  # Number of trees in the forest\n",
    "        'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "    },\n",
    "    'KNN': {\n",
    "        'n_neighbors': [3, 5, 7, 9],  # Number of neighbors to use\n",
    "        'weights': ['uniform', 'distance'],  # Weight function used in prediction\n",
    "        'metric': ['euclidean', 'manhattan']  # Distance metric for tree\n",
    "    },\n",
    "    'Decision Tree': {\n",
    "        'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "        'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "        'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [10, 50, 100, 200],  # Number of boosting stages to perform\n",
    "        'learning_rate': [0.01, 0.1, 0.2, 0.3],  # Learning rate\n",
    "        'max_depth': [3, 5, 8, 10],  # Maximum depth of the individual regression estimators\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "       \n",
    "        'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [10, 50, 100, 200],  # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.1, 0.2, 0.3],  # Boosting learning rate\n",
    "        'max_depth': [3, 5, 8, 10],  # Maximum tree depth\n",
    "        'min_child_weight': [1, 3, 5],  # Minimum sum of instance weight needed in a child\n",
    "        'gamma': [0, 0.1, 0.2, 0.3],  # Minimum loss reduction required to make a further partition\n",
    "        'subsample': [0.8, 0.9, 1.0],  # Subsample ratio of the training instances\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]  # Subsample ratio of columns when constructing each tree\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [10, 50, 100, 200],  # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.1, 0.2, 0.3],  # Boosting learning rate\n",
    "        'max_depth': [3, 5, 8, 10],  # Maximum tree depth\n",
    "        'num_leaves': [31, 50, 70, 100],  # Maximum tree leaves for base learners\n",
    "        'min_child_samples': [5, 10, 20, 30],  # Minimum number of data needed in a leaf\n",
    "        'subsample': [0.8, 0.9, 1.0],  # Subsample ratio of the training instances\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]  # Subsample ratio of columns when constructing each tree\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'n_estimators': [10, 50, 100, 200],  # Number of boosting rounds\n",
    "        'learning_rate': [0.01, 0.1, 0.2, 0.3],  # Boosting learning rate\n",
    "        'depth': [3, 5, 8, 10],  # Maximum tree depth\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9],  # Coefficient at the L2 regularization term\n",
    "        'bagging_temperature': [0, 0.5, 1, 1.5],  # Controls the intensity of Bayesian bagging\n",
    "        'border_count': [32, 64, 128, 254]  # Number of splits for numerical features\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
