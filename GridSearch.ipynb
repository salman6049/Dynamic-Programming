from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.model_selection import GridSearchCV


# Define classifiers
classifiers = {
    'logistic regression': LogisticRegression(random_state=42),
    'SVM': SVC(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'KNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(random_state=42),
    'XGBoost': XGBClassifier(random_state=42),
    'LightGBM': LGBMClassifier(random_state=42),
    'CatBoost': CatBoostClassifier(random_state=42, verbose=0)
}

# Define parameter grids for grid search
param_grids = {
    'logistic regression': {
        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]  # Inverse of regularization strength
    },
    'SVM': {
        'C': [0.1, 1, 10, 100],  # Regularization parameter
        'gamma': ['scale', 'auto', 0.1, 1, 10, 100],  # Kernel coefficient
        'kernel': ['linear', 'rbf', 'sigmoid']  # Kernel type
    },
    'Random Forest': {
        'n_estimators': [10, 50, 100, 200],  # Number of trees in the forest
        'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree
        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
        'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node
    },
    'KNN': {
        'n_neighbors': [3, 5, 7, 9],  # Number of neighbors to use
        'weights': ['uniform', 'distance'],  # Weight function used in prediction
        'metric': ['euclidean', 'manhattan']  # Distance metric for tree
    },
    'Decision Tree': {
        'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split
        'max_depth': [None, 10, 20, 30],  # Maximum depth of the tree
        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
        'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node
    },
    'Gradient Boosting': {
        'n_estimators': [10, 50, 100, 200],  # Number of boosting stages to perform
        'learning_rate': [0.01, 0.1, 0.2, 0.3],  # Learning rate
        'max_depth': [3, 5, 8, 10],  # Maximum depth of the individual regression estimators
        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node
       
        'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node
    },
    'XGBoost': {
        'n_estimators': [10, 50, 100, 200],  # Number of boosting rounds
        'learning_rate': [0.01, 0.1, 0.2, 0.3],  # Boosting learning rate
        'max_depth': [3, 5, 8, 10],  # Maximum tree depth
        'min_child_weight': [1, 3, 5],  # Minimum sum of instance weight needed in a child
        'gamma': [0, 0.1, 0.2, 0.3],  # Minimum loss reduction required to make a further partition
        'subsample': [0.8, 0.9, 1.0],  # Subsample ratio of the training instances
        'colsample_bytree': [0.8, 0.9, 1.0]  # Subsample ratio of columns when constructing each tree
    },
    'LightGBM': {
        'n_estimators': [10, 50, 100, 200],  # Number of boosting rounds
        'learning_rate': [0.01, 0.1, 0.2, 0.3],  # Boosting learning rate
        'max_depth': [3, 5, 8, 10],  # Maximum tree depth
        'num_leaves': [31, 50, 70, 100],  # Maximum tree leaves for base learners
        'min_child_samples': [5, 10, 20, 30],  # Minimum number of data needed in a leaf
        'subsample': [0.8, 0.9, 1.0],  # Subsample ratio of the training instances
        'colsample_bytree': [0.8, 0.9, 1.0]  # Subsample ratio of columns when constructing each tree
    },
    'CatBoost': {
        'n_estimators': [10, 50, 100, 200],  # Number of boosting rounds
        'learning_rate': [0.01, 0.1, 0.2, 0.3],  # Boosting learning rate
        'depth': [3, 5, 8, 10],  # Maximum tree depth
        'l2_leaf_reg': [1, 3, 5, 7, 9],  # Coefficient at the L2 regularization term
        'bagging_temperature': [0, 0.5, 1, 1.5],  # Controls the intensity of Bayesian bagging
        'border_count': [32, 64, 128, 254]  # Number of splits for numerical features
    }
}




def grid_search_classifiers(classifiers, param_grids, X_train, y_train):
    best_params = {}
    for name, clf in classifiers.items():
        grid_search = GridSearchCV(clf, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)
        grid_search.fit(X_train, y_train)
        best_params[name] = grid_search.best_params_
    return best_params



# Find the best parameters for each classifier
best_params = grid_search_classifiers(classifiers, param_grids, X_train, y_train)

# Update classifiers with the best parameters
for name, clf in classifiers.items():
    clf.set_params(**best_params[name])

# Evaluate classifiers with the new parameters
classifier_results = evaluate_classifiers(classifiers, X_train, X_test, y_train, y_test)

for name, accuracy in classifier_results.items():
    print(f"{name}: {accuracy:.2f}")
